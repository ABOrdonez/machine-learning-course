{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Aprendizaje no supervisado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Por qué? Cuándo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Cuando necesitamos que un modelo encuentre **estructura** en datos que **no están etiquetados (no tienen columnas \"de salida\")**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "La estructura a encontrar puede ser:\n",
    "\n",
    "- Agrupamiento de los datos (**clustering**).\n",
    "- Detección de **datos anómalos**.\n",
    "- Nuevas formas de expresar la información en menos features (**reducción de dimensionalidad**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Por lo general, hablamos de problemas donde los **humanos no pueden hacerlo** por si mismos, la data es demasiado grande (ej: encontrar agrupaciones de genes).\n",
    "\n",
    "Pero para **interpretar** los resultados, hace falta un humano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](files/images/ml_process.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Función resultante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "En aprendizaje **supervisado**, era claro que el resultado era una función que sabía predecir o clasificar.\n",
    "\n",
    "En aprendizaje **no supervisado**, no es tan claro. \n",
    "\n",
    "- Para clustering la salida puede ser simplemente la data agrupada. Puede o no dar una función que permita seguir agrupando.\n",
    "- Para detección de anomalías, la salida suele ser simplemente las anomalías detectadas. Puede o no dar una función que permita seguir detectando anomalías en nueva data.\n",
    "- Para reducción de dimensionalidad, suele ser la data reducida, mas una función que permita seguir reduciendo nuevos ejemplos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Tenemos un montón de **puntos**.\n",
    "\n",
    "Buscamos una forma de **agrupar esos puntos**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](files/images/unsupervised_learning_1.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](files/images/unsupervised_learning_2.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Los algoritmos no nos van a decir **por qué** esas cosas se agrupan, solo nos van a decir \"esto es parecido a esto, y diferente a esto otro\".\n",
    "\n",
    "Un experto del dominio luego puede intentar interpretar los grupos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Clustering: K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Es un algoritmo muy sencillo de clustering, que permite **agrupar** los datos en **K grupos**, siendo K un valor definido por nosotros.\n",
    "\n",
    "Lo logra buscando **centroides** que minimicen la distancia entre ellos y los ejemplos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](files/images/kmeans_step1.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](files/images/kmeans_step2.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Qué valores elegimos?\n",
    "\n",
    "- random entre rangos de los valores de nuestras features\n",
    "- random sacando K ejemplos de nuestra data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](files/images/kmeans_step3.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](files/images/kmeans_step4.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](files/images/kmeans_step5.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](files/images/kmeans_step6.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Resultado:\n",
    "\n",
    "- La data está agrupada!\n",
    "- Y tenemos los centroides para clasificar (determinar grupo) nueva data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Clustering: problemas de K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Problema 1: Qué valor **K** usamos??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](files/images/kmeans_k_small.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](files/images/kmeans_k_big.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Solución: podemos **iterar** y tratar de encontrar el mejor valor!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "... y cuál es el **\"mejor\"**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "El que tenga centroides con **menor distancia promedio** a los puntos de su grupo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "También en algunos casos, el **problema mismo** nos puede ayudar a decidir el valor para K. (ej: cuántos tipos de remeras podemos fabricar y seguir siendo rentables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Problema 2: máximos locales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](files/images/kmeans_local_max.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Solución: podemos iterar (muchas veces con el mismo K), y quedarnos con la **mejor** solución.\n",
    "\n",
    "**Mejor** = mismo criterio que antes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Problema 3: sets que no pueden clusterizarse con centroides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](files/images/kmeans_bad_set.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Solución: no hay :(\n",
    "\n",
    "Tenemos que usar algún **otro algoritmo**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ejemplo con ScikitLearn:\n",
    "\n",
    "```python\n",
    ">>> from sklearn.cluster import KMeans\n",
    ">>> X = np.array([[1, 2], [1, 4], [1, 0],\n",
    "...               [4, 2], [4, 4], [4, 0]])\n",
    ">>> kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    ">>> kmeans.labels_\n",
    "array([0, 0, 0, 1, 1, 1], dtype=int32)\n",
    ">>> kmeans.predict([[0, 0], [4, 4]])\n",
    "array([0, 1], dtype=int32)\n",
    ">>> kmeans.cluster_centers_\n",
    "array([[ 1.,  2.],\n",
    "       [ 4.,  2.]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Clustering: otros algoritmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](files/images/cluster_comparison.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering: Hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es una técnica bastante simple que va generando clusters agrupando o dividiendo clusters:\n",
    "\n",
    "* Aglomerativo: cada instancia arranca en su cluster y en cada iteración se unen los dos clusters mas cercanos\n",
    "* Divisivos: todas las instancias estan inicialmente en un cluster y luego se van dividiendo\n",
    "\n",
    "La forma más común de ver la salida de un cluster jerárquico es un **dendrograma**\n",
    "\n",
    "\n",
    "![](files/images/dendrogram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Tenemos un montón de **puntos**.\n",
    "\n",
    "Buscamos una forma de **representar esos puntos usando menos columnas**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Para qué?\n",
    "\n",
    "- **Acelerar** algoritmos de machine learning que usen esta data, y reducir sus **requerimientos de hardware** (menos columnas -> menos cálculos -> menos tiempo y memoria)\n",
    "- Poder **visualizar** información que tiene **demasiadas columnas** como para ser visualizada (reducir dimensiones mayores a 3, a algo de 1, 2 o 3 dimensiones)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](files/images/dim_reduction_1.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](files/images/dim_reduction_2.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](files/images/dim_reduction_3.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Las nuevas dimensiones no son **ninguna de las columnas originales**. Tienen un nuevo significado.\n",
    "\n",
    "En el ejemplo: no nos quedamos con X o con Y, inventamos dos nuevos nuevos ejes!\n",
    "\n",
    "Y uno de esos ejes, pareciera tener **poca información**... así que podríamos descartarlo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](files/images/dim_reduction_4.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Se **pierde información**. Nosotros debemos evaluar el trade off.\n",
    "\n",
    "Algunos de estos algoritmos permiten decidir de forma fácil **cuánta información perder vs cuántas dimensiones reducir** (ej: PCA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "En el ejemplo redujimos de 2 dimensiones originales, a 1 dimensión inventada. Estos algoritmos permiten ir de N a [algo menor que N] dimensiones. Ej: de 20 a 3 dimensiones, etc.\n",
    "\n",
    "Por lo general, cuantas más dimensiones reducimos, **más información perdemos**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Dimensionality Reduction: Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Es un algoritmo que permite hacer dimensionality reduction.\n",
    "\n",
    "No vamos a ver el trasfondo matemático, pero es bueno mencionarlo porque es **uno de los mas conocidos**, y simple con ScikitLearn:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    ">>> from sklearn.decomposition import PCA\n",
    ">>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    ">>> pca = PCA(n_components=2)\n",
    ">>> pca.fit(X)\n",
    "PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
    "  svd_solver='auto', tol=0.0, whiten=False)\n",
    ">>> print(pca.explained_variance_ratio_)  \n",
    "[ 0.99244...  0.00755...]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
