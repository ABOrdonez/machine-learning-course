{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "También conocidos como metaclasificadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Motivación:\n",
    "\n",
    "Machine Learning \"No free-lunch theorem\":\n",
    "\n",
    "- Distintos algoritmos funcionan correctamente en distintas situaciones\n",
    "\n",
    "- A priori no es factible saber qué algoritmo funcionará mejor\n",
    "\n",
    "- Por más que ajuste muy bien en el conjunto de test, aún puede estar fallando en algunos casos y podría existir otro algoritmo que a esos los ajuste mejor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A la hora de predecir, combinar distintas opiniones puede ser una excelente idea (como cuando vamos al médico) !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](files/images/Ensembles_example.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Formas de caracterización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Utilizando distintos espacios de predictoras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](files/images/Ensembles_type_1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Utilizando el mismo espacio de predictoras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](files/images/Ensembles_type_2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dependiendo de la forma de los clasificadores base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Homogeneos: conjunto de clasificadores de un mismo tipo (Ejemplo: todas redes neuronales)\n",
    "- Heterogeneos: conjunto de clasificadores de distintos tipos (Ejemplo: redes neuronales, k-NN, logistic regression, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dependiendo la estructura del ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Paralelos: Todos los clasificadores bases realizan una predicción y se combinan de alguna manera para obtener una única salida\n",
    "- Seriales: Se consultan los clasificadores bases de forma serial, donde cada uno recibe los datos de entrada y la salida del clasificador previo\n",
    "- Jerárquicos: Se establece una jerarquía y las salidas de los clasificadores base constituyen las entradas del metaclasificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Resumen parcial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Tenemos clasificadores base que sirven para obtener distintas opiniones sobre cada caso\n",
    "- Cada uno de ellos se ajusta mejor a una serie de casos y peor en otros\n",
    "- Lo ideal es combinar modelos sencillos\n",
    "- ** DEBE EXISTIR VARIABILIDAD EN LAS COMBINACIONES ! **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Métodos básicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Fusión de etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# 5 clasificadores, 3 etiquetas posibles.\n",
    "outputs = [[0,1,0], [1,0,0], [1,0,0], [0,1,0], [0,1,0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Voto por mayoría:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados parciales: [2 3 0]\n",
      "Etiqueta final: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('Resultados parciales:', np.sum(outputs, axis=0))\n",
    "print('Etiqueta final:', np.sum(outputs, axis=0).argmax(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Mayoría simple:\n",
    "\n",
    "La clase debe tener por lo menos la mitad de los votos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Voto por mayoría con umbral:\n",
    "La clase debe superar una cantidad mínima de votos, de lo contrario se utiliza una clase distinta de salida para indicar esta situación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Voto por mayoría ponderado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiquetas ponderadas: [array([0, 1, 0]), array([2, 0, 0]), array([2, 0, 0]), array([0, 1, 0]), array([0, 1, 0])]\n",
      "Resultados parciales: [4 3 0]\n",
      "Etiqueta final: 0\n"
     ]
    }
   ],
   "source": [
    "classifiers_weigth = [2,1,3]\n",
    "partial_sum = [np.array(o) * np.array(classifiers_weigth) for o in outputs]\n",
    "print('Etiquetas ponderadas:', partial_sum)\n",
    "print('Resultados parciales:', np.sum(partial_sum, axis=0))\n",
    "print('Etiqueta final:', np.sum(partial_sum, axis=0).argmax(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Usando probabilidades:\n",
    "- Media artimética\n",
    "- Minimo\n",
    "- Máximo\n",
    "- Mediana\n",
    "- Media ponderada\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Métodos avanzados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Bagging: Bootstrap AGGregatING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Es una metodología o forma de construir el metaclasificador (no un algoritmo en sí)\n",
    "- Se crean L conjuntos de datos a partir del conjunto inicial, para entrenar L clasificadores\n",
    "- Para predecir se combinan las salidas de los L clasificadores utilizando alguna técnica (generalmente voto por mayoría)\n",
    "- Cada conjunto de datos se genera utilizando \"boostrap\" (muestreo con reemplazamiento): algunas instancias van a quedar repetidas y otras se eliminarán\n",
    "- Tiene sentido con clasificadores base que sean inestables ! (pequeños cambios en los datos productes resultados diferentes)\n",
    "- Si tengo muchos datos los conjuntos probablemente sean similares y el método pierde eficacia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Procedimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Fase de entrenamiento:\n",
    "\n",
    "1) Para k clasificadores base, generar k conjuntos de datos con muestras ** boostrap ** a partir del conjunto de datos original \n",
    "\n",
    "2) Entrenar los k clasificadores, cada uno con uno de los conjuntos\n",
    "\n",
    "Fase de predicción:\n",
    "\n",
    "1) Clasificar la instancia con los k clasificadores\n",
    "\n",
    "2) Combinar las salidas y devolver un único resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](files/images/ensembles.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Es un algoritmo que utiliza Bagging\n",
    "- Ensemble homogeneo y paralelo: árboles como algoritmos base y creados de forma independiente\n",
    "- Introduce aleatoriedad extra: cada nodo del árbol se genera seleccionado un subconjunto aleatorio dentro de los atributos disponibles en cada momento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Hyper-parámetros para definir:\n",
    "\n",
    "- Cantidad de estimadores base\n",
    "- Porcentaje de variables a utilizar en cada split\n",
    "- Y todos los que teníamos para un árbol (profundidad máxima, cantidad de instancias mínimas en cada nodo hoja, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Algunas variantes ...\n",
    "\n",
    "- Pasting: cuando los conjuntos de datos se generan sin reemplazamiento y una cantidad de datos menor a la original.\n",
    "- Random Subspaces: cuando los conjuntos de datos se generan sobre un subconjunto de variables de entrada.\n",
    "- Random Patches: cuando lso conjuntos de datos se generan con las dos condiciones previas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Al igual que Bagging, es una metodología o forma de construir el metaclasificador (no un algoritmo en sí)\n",
    "- Los clasificadores base se construyen uno después del otro de forma incremental\n",
    "- La idea subyacente es que cada modelo se concentre más en las intancias donde el anterior falló\n",
    "- Se arranca entrenando un estimador base con una muestra boostrap, pero a partir del segundo conjunto de datos la probabilidad de seleccionar a cada instancia está condicionada al error previo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## AdaBoost (ADApting BOOSTint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Es un algoritmo que utiliza Boosting\n",
    "- Ensemble homogeneo y en serie: no está condicionado a un tipo en particular, pero el ensemble se compone de estimador de un mismo tipo, creados de a uno\n",
    "- Los estimadores base deben ser lo más simple posibles (no tiene sentido poner uno muy bueno de entrada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Procedimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Fase de entrenamiento:\n",
    "\n",
    "1) Inicializar los pesos de cada instancia en 1 / N (siendo N la cantidad de instancias)\n",
    "\n",
    "2) Generar una muestra boostrap utilizando los pesos de las instancias\n",
    "\n",
    "3) Entrenar un estimador con la muestra generada\n",
    "\n",
    "4) Calcular el error y actualizar los pesos de las instancias de acuerdo a ese error (a mayor error mayor peso)\n",
    "\n",
    "5) Repetir desde el paso 2 hasta que el error del modelo estimado esté por debajo de un umbral definido\n",
    "\n",
    "Fase de predicción:\n",
    "\n",
    "1) Utilizar todos los estimadores base entrenados en la fase de entrenamiento\n",
    "\n",
    "2) combinar las salidas utilizando la técnica de voto por mayoría ponderado (donde el peso de cada estimador está relacionado con el error obtenido en la fase de entrenamiento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Es otra variante que utiliza Boosting\n",
    "- Al igual que AdaBoost, también es homogeneo y en serie\n",
    "- Se basa en ir ajustando estimadores base utilizando las salidas del modelo previo como variable a predecir\n",
    "- Generalmente se utilizan árboles como estimadores base\n",
    "- Es uno de los modelos que más fama está teniendo últimamente (suele dar muy buenos resultados sin tener que preprocesar demasiado los datos)\n",
    "- xgboost es una de las librerías más utilizadas: tiene interfaces para varios lenguajes, está súper completa y optimizada !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Ejemplo:\n",
    "\n",
    "a) Vamos a entrenar a Franco para que prediga cuál va a ser la temperatura de mañana en base a la temperatura de hoy, el viento y la humedad. \n",
    "\n",
    "b) Cuando conocemos los errores de Franco a partir de una muestra, ahora le pedimos a Fisa que se aprenda los mismos ejemplos para predecir cuánto va a errar Franco en cada uno.\n",
    "\n",
    "En esa instancia la salida del modelo es lo que diga Franco + lo que estime Fisa.\n",
    "\n",
    "El algoritmo termina cuando el error es 0 y cae por debajo de un umbral definido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bagging vs. Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Similitudes:\n",
    "\n",
    "- Combinan estimadores base de un mismo tipo\n",
    "- Buscamos perder un poco de interpretabilidad para ganar en performance\n",
    "- Las salidas de combinan utilizando votos por mayoría (o alguna variante)\n",
    "- Todos los estimadores base se generan a partir del mismo conjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Diferencias:\n",
    "- Bagging permite paralelizar la construcción del ensemble mientras que Boosting no\n",
    "- Bagging aisla cada modelo construido, mientras que en Boosting el conocimiento se \"comparte\"\n",
    "- Boosting otorga pesos a los estimadores base mientras que ne Bagging todos cuenta por igual"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
